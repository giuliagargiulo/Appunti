---
author: Giulia Gargiulo
---

Come abbiamo visto, la memoria è fondamentale per svolgere le operazioni in un calcolatore; essa è un **vettore di parole o byte**, **ognuna col proprio indirizzo**.
La memoria vede soltanto un flusso di indirizzi di memoria, non sa come sono generati.

>[!Important]
>I dati che non sono in memoria devono essere caricati prima che la CPU possa operare su di essi.

I registri incorporati nella CPU sono accessibili nell'arco di un **clock** (un ciclo dell'orologio di sistema), mentre per accedere alla memoria centrale c'è bisogno di una transazione sul bus della memoria, per questo richiede più cicli di clock.

Nel caso in cui l'accesso in memoria centrale richieda molti clock, il processore entra necessariamente in stallo, poiché non ha i dati richiesti per completare l'istruzione che sta eseguendo. Questa situazione si può risolvere **inserendo una memoria veloce**, detta **cache**, tra la CPU e la memoria centrale, in modo da diminuire il tempo medio di accesso in memoria centrale.

Le principali **tecniche di gestione della memoria** sono 3:
- Allocazione contigua della memoria con partizioni multiple
- Paging
- Segmentazione

---
## Protezione della memoria
Un altro problema di cui tener conto è proteggere il sistema operativo dall'accesso dei processi utenti, e salvaguardare i processi utenti l'uno dall'altro. Infatti bisogna assicurarsi che **ciascun processo abbia uno spazio di memoria separato**, e bisogna poter determinare l'intervallo di indirizzi a cui ciascun processo può accedere, e garantire che possa accedere soltanto a questi indirizzi.

Il metodo più semplice è determinare un **registro base** e un **registro limite** (caricati solo dal SO). Il registro base contiene il più piccolo indirizzo legale della memoria fisica; il registro limite determina la dimensione dell'intervallo ammesso.
La CPU confronta ciascun indirizzo generato in modalità utente con i valori contenuti nei due registri; qualsiasi tentativo da parte di un programma utente di accedere ad aree di memoria riservate comporta l'invio di un segnare di eccezione che restituisce il controllo al SO, e generando un errore fatale.

Questo sistema impedisce ai programmi utenti di alterare il codice o le strutture dati di altri utenti o del SO.

![[Screenshot 2024-11-09 alle 17.28.32.png|400]]

---
## Binding degli indirizzi
In genere un programma risiede in un disco in forma di file eseguibile e, per essere eseguito, deve essere caricato in memoria ed inserito all'interno di un processo.
L'insieme dei processi presenti nei dischi e che attendono di essere trasferiti in memoria forma la coda d'ingresso, detta **input queue**.

In genere, si sceglie un processo dalla input queue e lo si carica in memoria; durante la sua esecuzione può accedere alle istruzioni e ai dati in memoria. Quando il processo termina, il suo spazio di memoria diventa disponibile per altri processi.

Generalmente, gli indirizzi di un programma sorgente sono simbolici. Un compilatore associa (bind) questi indirizzi simbolici ad indirizzi rilocabili. Il linkage editor o il loader fanno corrispondere questi indirizzi rilocabili ad indirizzi assoluti.
Ogni binding rappresenta una corrispondenza da uno spazio di indirizzi ad un altro.

Il binding può avvenire in una di queste 3 fasi:
1. **Compilazione:** se nella fase di compilazione si da dove il processo risiederà in memoria, si può generare codice assoluto; ad esempio se il processo utente inizia dalla locazione $r$, anche il codice generato inizierà dalla stessa locazione. Se la locazione cambia, bisogna ricompilare il codice.
2. **Caricamento:** se nella fase di compilazione non si sa in che punto della memoria risiederà il processo, è necessario generare codice rilocabile, ritardando il binding degli indirizzi alla fase di caricamento. Se l'indirizzo iniziale cambia, bisogna solo ricaricare il codice per aggiungere il valore modificato.
3. **Esecuzione:** Se durante l'esecuzione un processo può essere spostato da un segmento di memoria ad un altro, si deve ritardare il binding fino alla fase di esecuzione.
---
## Indirizzi logici e fisici
Un indirizzo generato dalla CPU è detto **indirizzo logico**, mentre un indirizzo visto dall'unità di memoria, cioè caricato nel registro dell'indirizzo di memoria è detto **indirizzo fisico**.
I metodi di binding degli indirizzi in fase di compilazione e caricamento producono indirizzi logici e fisici identici, mentre con i metodi di binding in fase di esecuzione, gli indirizzi logici (in questo caso detti virtuali) non coincidono con quelli fisici.

L'insieme degli indirizzi logici generati da un programma è detto spazio degli indirizzi logici, mentre l'insieme degli indirizzi fisici corrispondenti è detto spazio degli indirizzi fisici.

Il binding in fase di esecuzione dagli indirizzi virtuali agli indirizzi fisici è svolto dal memory-management unit (**MMU**). Quando un processo utente genera un indirizzo virtuale, prima dell'invio all'unità di memoria, si somma a tale indirizzo il valore contenuto nel registro di rilocazione (registro di base).
![[Screenshot 2024-11-09 alle 15.30.09.png|300]]
Il programma utente non considera mai gli indirizzi fisici reali, ma crea un puntatore alla locazione logica (ad esempio 346), lo memorizza, lo modifica, e solo quando esso assume il ruolo di indirizzo di memoria, si riloca il numero sulla base del contenuto del registro di rilocazione.

>[!important]
>Indirizzi logici $\rightarrow intervallo[0, max]$ 
>Indirizzi fisici $\rightarrow intervallo [0+r, max+r]$ con $r$ valore del registro di base.
>
>Un **processo utente genera solo indirizzi logici** che prima di essere usati, si devono far corrispondere a indirizzi fisici.

---
## Caricamento dinamico
Per migliorare l'utilizzo della memoria si può ricorrere al caricamento dinamico, mediante il quale si carica in memoria una procedura solo quando viene richiamata; tutte le procedure sono salvate in memoria in un formato di caricamento rilocabile e se durante l'esecuzione una procedura viene richiamata, si controlla che sia stata caricata, altrimenti viene caricata in memoria e viene aggiornata la tabella degli indirizzi del programma.

Il vantaggio è che **una procedura che non viene mai utilizzata non viene caricata**. Questo metodo è molto utile quando serve molto codice per gestire casi non frequenti.

---
## Linking statico e dinamico

#### Linking statico
Il **linking statico** avviene al momento della **compilazione**. Tutte le librerie e le dipendenze necessarie per eseguire il programma vengono integrate direttamente nel file eseguibile. Questo porta a file eseguibili di dimensioni maggiori, perché contengono tutto il codice necessario, ma offre i seguenti vantaggi:

- **Autonomia**: Il programma non ha bisogno di librerie esterne al momento dell’esecuzione, poiché tutte sono già integrate.
- **Stabilità**: Non ci sono rischi di incompatibilità dovuti a cambiamenti delle librerie nel tempo.

 Svantaggi:
- **Dimensione del file**: Gli eseguibili tendono a essere più grandi.
- **Aggiornamenti**: Se una libreria viene aggiornata, bisogna ricompilare l’intero programma per includere le modifiche.

#### Linking dinamico
Il linking dinamico, invece, avviene **al momento dell’esecuzione**. Le librerie e le dipendenze sono mantenute esternamente e vengono caricate solo quando il programma viene avviato. Questa tecnica è vantaggiosa per vari motivi:

- **Minore dimensione del file**: L’eseguibile non contiene tutto il codice delle librerie, quindi il file è più leggero.
- **Aggiornabilità**: Le librerie possono essere aggiornate indipendentemente dal programma, rendendo l'aggiornamento di singole parti più semplice. E' possibile caricare in memoria più di una versione della stessa libreria, ed è compito del programma controllare quale versione deve utilizzare. Inoltre, i programmi collegati prima dell'istallazione di una nuova libreria continuano ad utilizzare la vecchia libreria; questo sistema è noto con il nome di **librerie condivise**.

 Svantaggi:
- **Dipendenza da librerie esterne**: Se una libreria esterna viene rimossa o spostata, il programma potrebbe non funzionare.
- **Performance leggermente ridotta**: Dato che le librerie sono caricate solo durante l’esecuzione, può esserci un leggero ritardo all'avvio.
---
## Swapping
Per essere eseguito, un processo deve trovarsi in memoria centrale, ma se la sua esecuzione viene interrotta, il processo può essere temporaneamente trasferito in memoria ausiliaria, da cui verrà poi riportato in memoria centrale al momento di riprendere la sua esecuzione.

Ad esempio, si consideri un ambiente di multiprogrammazione con l'algoritmo Round Robin; trascorso un quanto di tempo, il gestore della memoria scarica dalla memoria centrale il processo appena terminato e ne carica uno nuovo nello spazio liberato. Nel frattempo lo sheduler della CPU assegna un quanto di tempo ad un altro processo, e quando termina lo scambia con un altro processo. Se lo scambio  è abbastanza rapido, i processi saranno già presenti in memoria e pronti per essere eseguiti quando lo sheduler riassegna la CPU. Questo procedimento è detto **swapping**. 

Di solito, un processo che è stato scaricato dalla memoria si deve ricaricare nello stesso spazio di memoria occupato in precedenza. Se l'associazione tra indirizzi logici e fisici si effettua nella fase di compilazione o caricamento, il processo non può essere ricaricato in posizioni diverse, mentre se si compie in fase di esecuzione si, poichè gli indirizzi fisici sono calcolati nella fase d'esecuzione.

---
## Allocazione contigua della memoria
La memoria centrale deve supportare sia i **processi di sistema** sia i **processi utente**; è necessario quindi assegnare le diverse parti della memoria centrale in modo efficiente.
La memoria centrale solitamente è divisa in due **partizioni**, una per il **sistema operativo** e una per i **processi utente**. Con l'allocazione contigua della memoria, ogni processo utente è contenuto in una singola sezione contigua di memoria.

>[!Nota]
>Grazie allo schema con registro di rilocazione descritto precedentemente, il sistema operativo può **cambiare dinamicamente le proprie dimensioni**.
>
>Ad esempio, il SO contiene codice e spazio di memoria per i driver dei dispositivi; se uno di questi, o altri servizi del SO, non sono comunemente utilizzati, possono essere scaricati dalla memoria, utilizzando lo spazio liberato per altri scopi.

### Allocazione con partizioni multiple

Uno dei modi più semplici per allocare memoria consiste nel dividere la memoria in **partizioni di dimensione variabile**, e ogni partizione deve contenere esattamente un processo. Il grado di multiprogrammazione in questo caso è limitato dal numero di partizioni. 

Quando una partizione è libera può essere occupata dal un processo presente nella **input queue.** Inizialmente tutta la memoria è disponibile, e quindi si ha un unico **hole** (cioè un unico grande blocco vuoto di memoria). 
Quando un processo arriva, viene inserito in un hole abbastanza grande per gestirlo; se un hole è troppo grande viene diviso in due parti. 
Quando il processo  termina, la sua memoria viene liberata e se ci sono hole adiacenti, si uniscono per formarne uno di dimensione maggiore.

In questo schema il SO conserva una **tabella** in cui sono indicate le partizioni libere (hole) e occupate.

A questo punto un problema fondamentale è scegliere quale hole assegnare ad un processo in attesa; i criteri più utilizzati sono i seguenti:
- **First-fit**: si assegna il **primo hole** abbastanza grande; la ricerca piò cominciare sia dall'inizio che dal punto in cui era terminata la ricerca precedente.
- **Best-fit**: si assegna il **più piccolo hole** che può contenere il processo. Richiede la ricerca in tutta la lista.
- **Worst-fit**: si assegna il **più grande hole** che può contenere il processo. Richiede la ricerca in tutta la lista. Si creano cosi parti di hole inutilizzate più grandi (per evitare la **frammentazione**).

Si è dimostrato che sia first-fit sia best-fit sono migliori rispetto a worst-fit in termini di risparmio di tempo e di utilizzo di memoria.

### Frammentazione

#### 1. Frammentazione esterna
Entrambi i criteri first-fit e best-fit di allocazione della memoria soffrono di **frammentazione esterna**: quando si caricano e si rimuovono i processi dalla memoria, si frammenta lo spazio libero della memoria in tante piccole parti. Si ha la frammentazione esterna se lo spazio di memoria totale è sufficiente per soddisfare una richiesta, ma non è contiguo; la memoria è frammentata in tanti piccoli hole.

Utilizzando il first-fit, per $n$ blocchi assegnati, si perdono altri $0,5n$ blocchi, e questo potrebbe rendere inutilizzabile un terzo della memoria (anche detta **50 percent rule**).

Una soluzione è data dalla **compattazione**, cioè la memoria viene riordinata per unire tutti gli hole in un unico blocco; essa può essere effettuata solo se la **rilocazione** è **dinamica** ed **avviene in fase di esecuzione**. Inoltre, la compattazione potrebbe richiedere costi elevati.

#### 2. Frammentazione interna
Si ha quando la memoria allocata è più grande della memoria richiesta, questo poichè se un hole è leggermente più grande della memoria richiesta dal processo, ad esempio richiesta 1000 byte e hole di 1002 byte, il carico necessario per tener traccia dell'hole che si crea è più grande dell'hole stesso (2 byte). La frammentazione interna consiste nella differenza tra la dimensione dell'hole e la richiesta (in questo caso 2 byte).

Per evitare questo problema, si divide la memoria in blocchi di dimensione fissa, così la memoria assegnata può essere leggermente maggiore della richiesta.

---
## Paging
Il paging è un metodo di gestione della memoria che permette che lo **spazio degli indirizzi fisici di un processo non sia contiguo**. Elimina quindi il problema della sistemazione dei blocchi in memoria ausiliaria.

Per implementare il paging, si suddivide la memoria fisica in blocchi di dimensione costante, detti **frame**, e la memoria logica in blocchi di uguale dimensione, detti **page** (pagine). Quando si deve eseguire un processo, si caricano le sue page nei frame disponibili, prendendole dalla memoria ausiliaria, divisa in blocchi di dimensione uguale a quella dei frame.

>[!Nota]
>Per lanciare un programma di dimensione N pagine, occorrono N frame liberi per caricare il programma in memoria.

Ogni indirizzo generato dalla CPU è diviso in due parti: page number($p$) e  page offset ($d$). 
- Il page number serve come indice per la **page table**, una tabella che si usa per convertire gli indirizzi logici in fisici e che contiene l'**indirizzo base** di ogni pagina nella memoria fisica.
- Il page offset combinato con l'indirizzo base definisce l'indirizzo di memoria fisica.
![[Screenshot 2024-11-10 alle 15.53.08.png|300]]
La dimensione di una page, cosi come quella di un frame, è definita dall'architettura del calcolatore, e in genere è una potenza di due compresa tra 512 byte e 16 Mb.
Se la dimensione dello spazio degli indirizzi logici è $2^m$, e la dimensione di un page è di $2^n$ (byte o parole), allora:
- page number = $m-n$ bit 
- page offset = $n$ bit

Con il paging si evita la frammentazione esterna, poichè ogni frame libero può essere assegnato ad un processo che ne ha bisogno; però può verificarsi la **frammentazione interna**, poichè lo spazio di memoria richiesto da un processo non è multiplo delle dimensioni delle pagine, quindi l'ultimo frame assegnato potrebbe essere non completamente pieno. Per ridurre al minino la frammentazione inetrna conviene utilizzare page di piccole dimensioni.

Un aspetto importante del paging è la distinzione tra la memoria vista dall'utente e l'effettiva memoria fisica. Il programma utente vede la memoria come un unico spazio contiguo, contenente solo il programma; in realtà, il programma è sparso nella memoria fisica, contenente anche altri programmi. 

Poiché il SO gestisce la memoria fisica, deve essere informato sull'allocazione della memoria: quali frame sono assegnati e quali liberi, il loro numero totale...
In genere queste informazioni sono contenute nella **tabella dei frame**, contenente un elemento per ogni frame, che indica se è libero o assegnato, e se è assegnato, a quale pagina di quale processo.

### Architettura di paging
L'architettura d'ausilio alla tabella delle pagine si può realizzare in vari modi. Il più semplice consiste in una serie di registri che operano ad alta velocità, ma questo metodo è efficiente solo se la tabella delle pagine è piccola, nell'ordine delle 256 entry. La maggior parte dei calcolatori però utilizza tabelle delle pagine dell'ordine di un milione di entry. Insieme alla tabella delle pagine, si mantiene in memoria anche il **page-table base register (PTBR)**, cioè il registro base della tabella delle pagine, che punta alla tabella stessa. Il cambio delle pagine richiede soltanto di modificare questo registro, riducendo considerevolmente i tempi di context-switch.

Questo metodo però presenta un problema connesso al tempo di accesso ad una locazione di memoria. Per accedere ad una locazione $i$, bisogna far riferimento alla tabella delle pagine utilizzando il valore contenuto nel PTBR aumentato del numero di pagina di $i$; si ottiene cosi il numero del frame, che associato all'offset di pagina, produce l'indirizzo cercato, permettendoci di accedere alla locazione desiderata.

Con questo metodo, **per accedere ad un byte occorrono due accessi in memoria** (uno per l'entry table e uno per il byte stesso), quindi l'accesso in memoria è lento.
La soluzione a questo problema è l'utilizzo di una piccola **cache molto veloce**, detta **TBL** in cui la ricerca è molto rapida, ma la memoria è molto costosa.

#### TBL
La TBL è una memoria associativa in cui ogni sua entry è formata da due parti: una chiave e un valore; essa contiene una piccola parte delle entry della page table. Quando la CPU genera un indirizzo logico, il suo numero di pagina viene passato alla TBL, che lo confronta contemporaneamente con tutte le chiavi presenti in memoria; se il numero è presente, il corrispondente numero del frame è immediatamente disponibile e si usa per accedere alla memoria, altrimenti si verifica un **TBL miss**, cioè il valore non è stato trovato, e si deve consultare la page table. Inserendo però i numeri della pagina e del frame nella TBL, al prossimo accesso allo stesso elemento la ricerca sarà molto più rapida.

La TBL assicura anche la protezione dello spazio degli indirizzi di un processo, conservando gli address-space-identifier (ASID) in ciascuna entry. La TBL assicura che l'ASID per il processo attualmente in esecuzione corrisponda all' ASID associato alla pagina virtuale. La mancata corrispondenza dell’ASID viene trattata come una TLB miss.

>[!Nota]
>Per calcolare il tempo effettivo di accesso in memoria utilizzando la TBL si utilizza la seguente formula:
>$$EAT = (T_a + \varepsilon) \alpha  + (2T_a + \varepsilon)(1 - \alpha)$$
>- $T_a$ è il tempo di accesso in memoria centrale
>- $\varepsilon$ è il tempo di ricerca nella TBL 
>- $\alpha$ è l'**hit ratio**.

#### Protezione
In un ambiente paginato, la protezione della memoria è data dai bit di protezione associati ad ogni frame nella tabella delle pagine. Un bit può determinare se una pagina si può leggere e scrivere o soltanto leggere. Mentre si calcola l'indirizzo fisico si può controllare il bit di protezione e verificare che non si scriva su una pagina di sola lettura; un tentativo di questo tipo causerebbe l'invio di un segnare di eccezione al SO.

Di solito si associa ad ogni elemento nella tabella delle pagine un ulteriore bit, detto **valid-invalid bit**, che se è impostato a **valid** indica chela pagina corrispondente si trova nello spazio degli indirizzi logici del processo, e quindi vi si può accedere, altrimenti se è impostato a **invalid**, indica che non vi si può accedere, e viene generato un segnale di eccezione.

Inoltre, capita raramente che un processo utilizzi tutto il suo intervallo di indirizzi, quindi una tabella troppo grande sarebe inutile; per questo alcune architetture dispongono di registri, detti **page-table lenght register (PTLR)**, per indicare le dimensioni della tabella delle pagine.

#### Pagine condivise
Un **vantaggio** del paging consiste nel poter condividere tra più processi codice comune. Il codice può essere condiviso solo se è **rientrante**, cioè che **non cambia durante l'esecuzione**. Quindi due o più processi possono eseguire lo stesso codice contemporaneamente, ma ciascun processo possiede la propria copia dei registri e di una memoria dove conservare i dati necessari alla propria esecuzione. 

Un esempio potrebbe essere un editor di testo di tre pagine, condiviso da 3 processi, ciascuno dei quali possiede la propria pagina di dati.

![[Screenshot 2024-11-10 alle 19.43.10.png|400]]
#### Struttura della tabella delle pagine
I moderni calcolatori dispongono di uno spazio di indirizzi logici molto grandi (da $2^{32}$ a $2^{64}$ elementi), quindi la tabella delle pagine diventerebbe eccessivamente grande.
Ad esempio, in un sistema con spazio di indirizzi logici a 32 bit con pagine di 4 Kb, la tabella della pagine potrebbe contenere fino a $2^{32}/ 2^{12} = 2^{20}$ elementi. Inoltre sarebbe meglio non allocare la tabella della pagine in modo contiguo in memoria centrale.
Una soluzione consiste nell'adottare un algoritmo di paginazione a due livelli, in cui la tabella stessa è paginata. Si consideri il precedente esempio di una macchina a 32 bit con dimensione delle pagine di 4 Kb. Si suddivide ciascun indirizzo logico in un numero di pagina di 20 bit e un offset di 12 bit ($d$); a sua volta il numero di pagina si suddivide ulteriormente in un numero di pagina di 10 bit ($p_1$) e un offset di 10 bit($p_2$).
![[Screenshot 2024-11-10 alle 20.45.35.png|300]]
- $p_1$ è l'indice della tabella di primo livello (o tabella esterna).
- $p_2$ è l'offset della pagina indicata dalla tabella di primo livello delle pagine.

![[Screenshot 2024-11-10 alle 20.48.34.png|400]]

>[!Nota]
>Per una macchina a 64 bit potrebbe essere necessaria una paginazione a 3 livelli.

#### Tabella delle pagine invertita
Generalmente, si utilizza una tabella delle pagine per ogni processo, e questa tabella contiene un elemento per ogni pagina virtuale che il processo sta utilizzando. Uno dei problemi di questo metodo è che ogni tabella delle pagine può contenere milioni di elemento e occupare molto spazio in memoria.

Per risolvere questo problema si utilizza la **tabella della pagine invertita**, che ha un elemento per ogni pagina reale(frame). Ciascun elemento è costituito dall'indirizzo virtuale della pagina memorizzata in una specifica locazione reale di memoria, con informazioni sul processo che possiede tale pagina.
Quindi nel sistema esiste una sola tabella delle pagine, che ha un solo elemento per ogni frame.
![[Screenshot 2024-11-10 alle 17.35.15.png|300]]
Le tabelle delle pagine invertite spesso richiedono la memorizzazione di un ASID in ogni elemento della tabella, poiché essa di solito contiene molti spazi d'indirizzi diversi associati alla memoria fisica. Un indirizzo virtuale è una tripla $(process\_id,page\_number, offset)$ , mentre un elemento della tabella delle pagine invertita è una coppia $(process\_id,page\_number)$, dove il $process\_id$ assume il ruolo di ASID. Quando si cerca una corrispondenza nella tabella delle pagine invertita, se si trova la corrispondenza su un elemento $i$, si genera l'indirizzo fisico $(i, offset)$; in caso contrario è stato tentato un accesso illegale ad un indirizzo.

>[!Important]
>Sebbene la tabella delle pagine invertite riduca la quantità di memoria necessaria, aumenta però il tempo di ricerca nella tabella, infatti per trovare una corrispondenza bisogna esaminare tutta la tabella. Per limitare il tempo di ricerca si può utilizzare una tabella hash.

---
### Segmentazione